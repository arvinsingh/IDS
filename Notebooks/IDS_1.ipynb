{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 14}\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# SET PATH\n",
    "PATH = 'Dataset/NSL-KDD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle as pkl\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataframe and get it ready for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ids_dataframe_nor.pkl\", 'rb') as pkld:\n",
    "    normal_df = pkl.load(pkld)\n",
    "with open(\"ids_dataframe.pkl\", 'rb') as pkld:\n",
    "    main_df = pkl.load(pkld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0=normal, 1=DoS, 2=Probe, 3=R2L and 4=U2R.\n",
    "normal_df = main_df[main_df['label'] == 0]\n",
    "dos_df = main_df[main_df['label'] == 1]\n",
    "probe_df = main_df[main_df['label'] == 2]\n",
    "r2l_df = main_df[main_df['label'] == 3]\n",
    "u2r_df = main_df[main_df['label'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dos_df = shuffle(pd.concat([normal_df, dos_df], axis=0))\n",
    "n_dos_df.reset_index(drop=True, inplace=True)\n",
    "n_probe_df = shuffle(pd.concat([normal_df, probe_df], axis=0))\n",
    "n_probe_df.reset_index(drop=True, inplace=True)\n",
    "n_r2l_df = shuffle(pd.concat([normal_df, r2l_df], axis=0))\n",
    "n_r2l_df.reset_index(drop=True, inplace=True)\n",
    "n_u2r_df = shuffle(pd.concat([normal_df, u2r_df], axis=0))\n",
    "n_u2r_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_dos = n_dos_df[\"label\"]\n",
    "targets_n_probe = n_probe_df[\"label\"]\n",
    "targets_n_r2l = n_r2l_df[\"label\"]\n",
    "targets_n_u2r = n_u2r_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dos_df.drop(columns=[\"label\"], inplace=True)\n",
    "n_probe_df.drop(columns=[\"label\"], inplace=True)\n",
    "n_r2l_df.drop(columns=[\"label\"], inplace=True)\n",
    "n_u2r_df.drop(columns=[\"label\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate grid search with the model training to check for multiple algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skfold(X, y, model):\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    skf.get_n_splits(X, y)\n",
    "    results = cross_val_score(model, X, y, cv=skf)\n",
    "    return results\n",
    "\n",
    "\n",
    "xgb = skfold(X, y, xgboost.XGBClassifier())\n",
    "gnb = skfold(X, y, GaussianNB())\n",
    "rf = skfold(X, y,RandomForestClassifier())\n",
    "svc = skfold(X, y,SVC())\n",
    "ada = skfold(X, y,AdaBoostClassifier())\n",
    "knn = skfold(X, y,KNeighborsClassifier())\n",
    "MLPc = skfold(X, y,MLPClassifier())\n",
    "dic = {'XGB':xgb, 'RandomForest':rf,'SVC':svc,'GaussianNB':gnb,'adaBoost':ada,'KNN':knn, 'MLPc':MLPc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model by grid search with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    grid = GridSearchCV(estimator=xgb_, param_grid=params, scoring='roc_auc',\\n                        n_jobs=8, cv=skf.split(X_train, Y_train), verbose=3)\\n    grid.fit(X_train, Y_train)\\n    print('\\n Best estimator:')\\n    print(grid.best_estimator_)\\n    print('\\n Best score:')\\n    print(grid.best_score_ * 2 - 1)\\n    print('\\n Best parameters:')\\n    print(grid.best_params_)\\n \""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cv_hparam_tune(data, targets):\n",
    "    # A parameter grid for XGBoost\n",
    "\n",
    "    params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5, 6, 7]\n",
    "    }\n",
    "\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data, targets, test_size=0.20)\n",
    "\n",
    "\n",
    "    xgb_ = xgboost.XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                                silent=True, nthread=1)\n",
    "\n",
    "\n",
    "    folds = 5\n",
    "    param_comb = 5\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1001)\n",
    "\n",
    "    random_search = RandomizedSearchCV(xgb_, param_distributions=params, n_iter=param_comb,\n",
    "                                    scoring='roc_auc', n_jobs=8, cv=skf.split(X_train, Y_train),\n",
    "                                    verbose=3, random_state=1001\n",
    "                                    )\n",
    "\n",
    "    # Here we go\n",
    "    random_search.fit(X_train, Y_train)\n",
    "    print('\\n Best estimator:')\n",
    "    print(random_search.best_estimator_)\n",
    "    print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' %\n",
    "        (folds, param_comb))\n",
    "    print(random_search.best_score_ * 2 - 1)\n",
    "    print('\\n Best hyperparameters:')\n",
    "    print(random_search.best_params_)\n",
    "\n",
    "\"\"\"    grid = GridSearchCV(estimator=xgb_, param_grid=params, scoring='roc_auc',\n",
    "                        n_jobs=8, cv=skf.split(X_train, Y_train), verbose=3)\n",
    "    grid.fit(X_train, Y_train)\n",
    "    print('\\n Best estimator:')\n",
    "    print(grid.best_estimator_)\n",
    "    print('\\n Best score:')\n",
    "    print(grid.best_score_ * 2 - 1)\n",
    "    print('\\n Best parameters:')\n",
    "    print(grid.best_params_)\n",
    " \"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  19 out of  25 | elapsed: 10.8min remaining:  3.4min\n",
      "[Parallel(n_jobs=8)]: Done  25 out of  25 | elapsed: 13.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best estimator:\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.8, gamma=0.5, learning_rate=0.02,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
      "       n_estimators=600, n_jobs=1, nthread=1, objective='binary:logistic',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=1.0)\n",
      "\n",
      " Best normalized gini score for 5-fold search with 5 parameter combinations:\n",
      "0.9999986899617652\n",
      "\n",
      " Best hyperparameters:\n",
      "{'subsample': 1.0, 'min_child_weight': 1, 'max_depth': 3, 'gamma': 0.5, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "cv_hparam_tune(n_dos_df, targets_dos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
